1+ 1
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
beta
alpha
posterior(alpha, beta)
posterior(alpha_sample, beta_sample)
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
beta
alpha
posterior(alpha, beta)
posterior(alpha_sample, beta_sample)
x = 1:989
length(x)
length(alphas)
plot(x, alphas)
y = 1:989
length(y)
length(betas)
plot(y, betas)
library(boot)
x <- c(-.86, -.30, -.05, .73)
y <- c(0, 1, 3, 5)
n <- c(5, 5, 5, 5)
sigma.a <- 1  # Need to play around with these values!
sigma.b <- 3  # Need to play around with these values!
alpha0 <- 0.8
beta0 <- 7.7
chain.length <- 2000
library(boot)
x <- c(-.86, -.30, -.05, .73)
y <- c(0, 1, 3, 5)
n <- c(5, 5, 5, 5)
sigma.a <- 1  # Need to play around with these values!
sigma.b <- 3  # Need to play around with these values!
alpha0 <- 0.8
library(boot)
x <- c(-.86, -.30, -.05, .73)
y <- c(0, 1, 3, 5)
n <- c(5, 5, 5, 5)
sigma.a <- 1  # Need to play around with these values!
sigma.b <- 3  # Need to play around with these values!
alpha0 <- 0.8
beta0 <- 7.7
log.post <- function(alpha, beta, n, x, y){
theta <- inv.logit(alpha + beta*x)
posterior <- sum(dbinom(y, n, theta, log=TRUE))
posterior
}
chain <- build.chain(alpha0, beta0, sigma.a, sigma.b, chain.length, n, x, y)
alpha.chain <- chain$alpha.chain
load("C:/Users/chenx/Downloads/demo11_2a.RData")
setwd("C:/Users/chenx/OneDrive/Desktop/LDA_twitter_sentiment/different_approach")
packages <- c("dplyr",
"magrittr",
"data.table",
"tidytext",
"topicmodels",
"colorspace",
"purrr",
"ldatuning",
"gmp",
"wordcloud",
"RColorBrewer",
"ggplot2",
"lubridate","reshape2","textmineR")
# if need to install packages
for (i in 2:length(packages))
{
install.packages(packages[i])
}
packages <- c("dplyr",
"magrittr",
"data.table",
"tidytext",
"topicmodels",
"colorspace",
"purrr",
"ldatuning",
"gmp",
"wordcloud",
"RColorBrewer",
"ggplot2",
"lubridate","reshape2","textmineR")
# if need to install packages
for (i in 2:length(packages))
{
install.packages(packages[i])
}
install.packages(packages[i])
install.packages(packages[i])
packages <- c("dplyr",
"magrittr",
"data.table",
"tidytext",
"topicmodels",
"colorspace",
"purrr",
"ldatuning",
"gmp",
"wordcloud",
"RColorBrewer",
"ggplot2",
"lubridate","reshape2","textmineR")
lapply(packages, require, character.only = TRUE)
data <- fread("Sentiment.csv")
data <- data %>% select(text,id) %>% head(5000)
# pre-processing
data$text <- sub("RT.*:", "", data$text)
data$text <- sub("@.* ", "", data$text)
text_cleaning_tokens <- data %>%
tidytext::unnest_tokens(word, text)
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens <- text_cleaning_tokens %>% filter(!(nchar(word) == 1))%>%
anti_join(stop_words)
tokens <- text_cleaning_tokens %>% filter(!(word==""))
tokens <- tokens %>% mutate(ind = row_number())
tokens <- tokens %>% group_by(id) %>% mutate(ind = row_number()) %>%
tidyr::spread(key = ind, value = word)
tokens [is.na(tokens)] <- ""
tokens <- tidyr::unite(tokens, text,-id,sep =" " )
tokens$text <- trimws(tokens$text)
View(tokens)
View(tokens)
View(tokens)
View(text_cleaning_tokens)
View(data)
