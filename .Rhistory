1+ 1
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
beta
alpha
posterior(alpha, beta)
posterior(alpha_sample, beta_sample)
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
library(logitnorm)
library(MASS)
library(mvtnorm)
library(boot)
likelihood <- function(alpha, beta, x, n, y) {
theta <- alpha + beta * x
print(invlogit(theta))
result <- y * log(invlogit(theta)) + (n - y) * log(1 - invlogit(theta))
result
}
posterior <- function(alpha, beta){
result <- likelihood(alpha, beta, -0.86, 5, 0) +
likelihood(alpha, beta, -3.0, 5, 1) +
likelihood(alpha, beta, -0.05, 5, 3) +
likelihood(alpha, beta, 0.73, 5, 5)
result
}
alpha <- 0.8
beta <- 7.7
points <- list()
alphas <- list()
betas <- list()
inv.logit(-100)
for (i in 1:10000) {
sample_var <- MASS::mvrnorm(n=1, mu = c(alpha,beta), Sigma = diag(2))
alpha_sample <- sample_var[1]
beta_sample <- sample_var[2]
r <- exp((posterior(alpha_sample, beta_sample) * dmvnorm(c(alpha, beta), mean = c(alpha_sample, beta_sample), sigma = diag(2))) - (posterior(alpha, beta) * dmvnorm(c(alpha_sample, beta_sample), mean = c(alpha, beta), sigma = diag(2))))
print(r)
if (r >= 1) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
} else {
sample_num <- runif(1)
if (sample_num < r) {
alpha <- sample_var[1]
beta <- sample_var[2]
points <- append(points, c(alpha, beta))
alphas <- append(alphas, alpha)
betas <- append(betas, beta)
}
}
}
beta
alpha
posterior(alpha, beta)
posterior(alpha_sample, beta_sample)
x = 1:989
length(x)
length(alphas)
plot(x, alphas)
y = 1:989
length(y)
length(betas)
plot(y, betas)
library(boot)
x <- c(-.86, -.30, -.05, .73)
y <- c(0, 1, 3, 5)
n <- c(5, 5, 5, 5)
sigma.a <- 1  # Need to play around with these values!
sigma.b <- 3  # Need to play around with these values!
alpha0 <- 0.8
beta0 <- 7.7
chain.length <- 2000
library(boot)
x <- c(-.86, -.30, -.05, .73)
y <- c(0, 1, 3, 5)
n <- c(5, 5, 5, 5)
sigma.a <- 1  # Need to play around with these values!
sigma.b <- 3  # Need to play around with these values!
alpha0 <- 0.8
library(boot)
x <- c(-.86, -.30, -.05, .73)
y <- c(0, 1, 3, 5)
n <- c(5, 5, 5, 5)
sigma.a <- 1  # Need to play around with these values!
sigma.b <- 3  # Need to play around with these values!
alpha0 <- 0.8
beta0 <- 7.7
log.post <- function(alpha, beta, n, x, y){
theta <- inv.logit(alpha + beta*x)
posterior <- sum(dbinom(y, n, theta, log=TRUE))
posterior
}
chain <- build.chain(alpha0, beta0, sigma.a, sigma.b, chain.length, n, x, y)
alpha.chain <- chain$alpha.chain
load("C:/Users/chenx/Downloads/demo11_2a.RData")
Sys.which("make")
install.packages("jsonlite", type = "source")
library(dplyr)
library(tidytext)
library(tidyverse)
library(tm)
library(stringr)
install.packages("ggplot2", type = "source")
install.packages("ggplot2", type = "source")
library(tm)
library(stringr)
library(rstan)
library(shinystan)
library(rstan)
library(shinystan)
# balance unfair 0 and 1
data <- read.csv("train_tweets.csv")
setwd("C:/Users/chenx/OneDrive/Desktop/LDA_twitter_sentiment")
data
# balance unfair 0 and 1
data <- read.csv("train_tweets.csv")
preprocess_data_1 <- filter(data, data$label == 1)
dim(preprocess_data_1)
preprocess_data_0 <- filter(data, data$label == 0)
dim(preprocess_data_0)
preprocess_data_0 <- preprocess_data_0[1:2242, ]
dim(preprocess_data_0)
train_set <- rbind(preprocess_data_0, preprocess_data_1)
set.seed(499)
sample_rows <- sample(nrow(train_set))
shuffle_train_set <- train_set[sample_rows, ]
train_set = shuffle_train_set[1: 4000, ]
test_set = shuffle_train_set[4001:4484, ]
train_set$id = 1:4000
id = train_set$id
# pre-processing
train_set$tweet <- gsub("#", "", train_set$tweet)
train_set$tweet <- gsub("@", "", train_set$tweet)
train_set$tweet <- gsub('[[:digit:]]+', '', train_set$tweet)
train_set$tweet <- gsub('[[:punct:]]+', '', train_set$tweet)
train_set$tweet = removeWords(train_set$tweet, stopwords("english"))
train_set$tweet = stripWhitespace(train_set$tweet)
text_cleaning_tokens <- train_set %>%
tidytext::unnest_tokens(word, tweet)
hist(text_cleaning_tokens$label)
dim(text_cleaning_tokens)
# model
K = 2 # num of topics
V = 37282 # num of words
M = 4000 # num docs
N = length(unique(text_cleaning_tokens$word))
w = unique(text_cleaning_tokens$word)
doc = array()
alpha = rep(1, 2)
beta = rep(1, V)
model <- stan_model('LDA_Model.stan')
rstan_options(javascript = FALSE)
model <- stan_model('LDA_Model.stan')
list_words = list()
words = array()
for (i in 1:N) {
if (w[i] %in% list_words) {
index = match(w[i], list_words)
words[i] = index
print(1)
print(w[i])
} else {
print(2)
length_word = length(list_words) + 1
print(w[i])
list_words[length_word] = w[i]
words[i] = length_word
}
}
for (i in 1:N) {
index = match(list_words[i], text_cleaning_tokens$word)
doc_id = text_cleaning_tokens[index, ]$id
print(index)
print(doc_id)
doc[i] = doc_id
}
# fit
options(mc.cores=4)
fit <- sampling(model, list(K=K, V=V, M=M, N=N, w=words, doc=doc, alpha=alpha, beta=beta), iter=2000, chains=4)
# visualize
shinystan(fit)
# visualize
launch_shinystan(fit)
gc()
View(fit)
View(fit)
View(fit)
